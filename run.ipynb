{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fd7d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Complete Causal Inference Pipeline for Diagnostic Escalation\n",
    "# \n",
    "# This notebook walks through the entire pipeline, from loading the raw ARS data to estimating the causal effect of resistance on diagnostic escalation, comparing standard and joint selection models, applying Bayesian shrinkage, and exploring heterogeneity with causal forests.\n",
    "# \n",
    "# **Prerequisites**: \n",
    "# - Python environment with `escalation-causal` installed.\n",
    "# - ARS dataset in Parquet format (adjust the path below).\n",
    "# - Filter configuration JSON file (adjust path).\n",
    "# \n",
    "# **Notation**:\n",
    "# - \\(T_{code}\\): tested indicator (1 if tested, 0 otherwise)\n",
    "# - \\(R_{code}\\): resistance indicator (1 if tested and resistant, 0 otherwise)\n",
    "# - \\(A\\): trigger antibiotic (treatment)\n",
    "# - \\(D\\): target antibiotic (outcome)\n",
    "# - \\(C\\): context (lab, pathogen group, year)\n",
    "# - \\(Y^*_D\\): escalation score for target \\(D\\)\n",
    "# - \\(\\psi\\): risk difference \\(\\mathbb{E}[Y^*|A=1] - \\mathbb{E}[Y^*|A=0]\\)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 1. Setup and Imports\n",
    "\n",
    "# %%\n",
    "import sys\n",
    "import logging\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GroupShuffleSplit\n",
    "\n",
    "# Our package modules\n",
    "from src.controllers.DataLoader import DataLoader\n",
    "from src.controllers.filters.FilteringStrategy import FilterConfig\n",
    "from src.controllers.escalation_causal.config.settings import (\n",
    "    RunConfig, SplitConfig, CovariateConfig, PolicyConfig, NuisanceConfig, TMLEConfig\n",
    ")\n",
    "from src.controllers.escalation_causal.pipeline import CausalPipeline\n",
    "from src.controllers.escalation_causal.screening.phase1_screener import Phase1Screener, Phase1Config\n",
    "from src.controllers.escalation_causal.utils.io import save_results\n",
    "from src.controllers.escalation_causal.nuisance.joint_selection import JointSelectionModel\n",
    "from src.controllers.escalation_causal.multiple_comparison.bayesian_shrinkage import BayesianShrinkage\n",
    "from src.controllers.escalation_causal.heterogeneity.causal_forest import CausalForestWrapper\n",
    "\n",
    "# Plotting\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "816785ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "APPLYING FILTERS\n",
      "================================================================================\n",
      "[Pathogen:equals] 328,914 → 26,765 (8.1% retained, 302,149 dropped)\n",
      "[CSQMG:equals] 26,765 → 21,937 (82.0% retained, 4,828 dropped)\n",
      "[ARS_WardType:in] 21,937 → 21,462 (97.8% retained, 475 dropped)\n",
      "[CareType:in] 21,462 → 21,462 (100.0% retained, 0 dropped)\n",
      "[Year:range] 21,462 → 21,462 (100.0% retained, 0 dropped)\n",
      "\n",
      "================================================================================\n",
      " FILTERING SUMMARY\n",
      "================================================================================\n",
      "Initial rows:     328,914\n",
      "Final rows:       21,462\n",
      "Total removed:    307,452\n",
      "Overall retained: 6.5%\n",
      "================================================================================\n",
      "\n",
      "\n",
      "================================================================================\n",
      "APPLYING FILTERS (All Isolates for Klebsiella pneumoniae Analysis)\n",
      "================================================================================\n",
      "[exclusions] dropped=0 detail={'IsSpecificlyExcluded_Screening': 0, 'IsSpecificlyExcluded_Pathogen': 0, 'IsSpecificlyExcluded_PathogenevidenceNegative': 0}\n",
      "[Pathogen:equals] 328,914 → 26,765 (8.1% retained, 302,149 dropped)\n",
      "[CSQMG:equals] 26,765 → 21,937 (82.0% retained, 4,828 dropped)\n",
      "[ARS_WardType:in] 21,937 → 21,462 (97.8% retained, 475 dropped)\n",
      "[CareType:in] 21,462 → 21,462 (100.0% retained, 0 dropped)\n",
      "[Year:range] 21,462 → 21,462 (100.0% retained, 0 dropped)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "✅ Final cohort: 21462 isolates from 77 laboratories\n",
      "   Year range: 2020-01 – 2022-12\n",
      "Total antibiotic codes: 61\n",
      "Flags shape: (21462, 122)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AMC_T</th>\n",
       "      <th>AMC_R</th>\n",
       "      <th>AMK_T</th>\n",
       "      <th>AMK_R</th>\n",
       "      <th>AMP_T</th>\n",
       "      <th>AMP_R</th>\n",
       "      <th>AMS_T</th>\n",
       "      <th>AMS_R</th>\n",
       "      <th>AMX_T</th>\n",
       "      <th>AMX_R</th>\n",
       "      <th>...</th>\n",
       "      <th>TGC_T</th>\n",
       "      <th>TGC_R</th>\n",
       "      <th>TOB_T</th>\n",
       "      <th>TOB_R</th>\n",
       "      <th>TPL_T</th>\n",
       "      <th>TPL_R</th>\n",
       "      <th>TRP_T</th>\n",
       "      <th>TRP_R</th>\n",
       "      <th>VAN_T</th>\n",
       "      <th>VAN_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 122 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   AMC_T  AMC_R  AMK_T  AMK_R  AMP_T  AMP_R  AMS_T  AMS_R  AMX_T  AMX_R  ...  \\\n",
       "0      1      0      1      0      1      1      0      0      0      0  ...   \n",
       "1      1      0      1      0      1      1      0      0      0      0  ...   \n",
       "2      1      0      1      0      1      1      0      0      0      0  ...   \n",
       "3      1      0      1      0      1      1      0      0      0      0  ...   \n",
       "4      1      0      1      0      1      1      0      0      0      0  ...   \n",
       "\n",
       "   TGC_T  TGC_R  TOB_T  TOB_R  TPL_T  TPL_R  TRP_T  TRP_R  VAN_T  VAN_R  \n",
       "0      0      0      1      0      0      0      1      0      0      0  \n",
       "1      0      0      1      0      0      0      1      0      0      0  \n",
       "2      0      0      1      0      0      0      1      0      0      0  \n",
       "3      0      0      1      0      0      0      1      0      0      0  \n",
       "4      0      0      1      0      0      0      1      0      0      0  \n",
       "\n",
       "[5 rows x 122 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 2. Load and Filter Real Data\n",
    "# \n",
    "# We use the `DataLoader` to read the ARS Parquet file and apply the inclusion/exclusion criteria defined in a JSON configuration file.\n",
    "# \n",
    "# **Why this step?**  \n",
    "# Raw data often contain screening cultures, repeated isolates, or other records that are not suitable for causal analysis. Filtering ensures we work with a clean, clinically relevant cohort.\n",
    "# \n",
    "# **What if you skip filtering?**  \n",
    "# You might include screening cultures or repeated isolates, violating the independence assumption and biasing your results.\n",
    "\n",
    "# %%\n",
    "data_path = \"./datasets/structured/dataset_parquet\"          # <-- UPDATE THIS PATH\n",
    "filter_config_path = \"./src/controllers/filters/config_all.json\"  # <-- UPDATE THIS PATH\n",
    "\n",
    "loader = DataLoader(data_path, strict=False, normalize_on_load=True)\n",
    "filter_config = FilterConfig.from_json(filter_config_path)\n",
    "\n",
    "df, meta = loader.get_cohort(\n",
    "    filter_config=filter_config,\n",
    "    apply_exclusions=True,\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "print(f\"\\n✅ Final cohort: {len(df)} isolates from {meta.n_labs} laboratories\")\n",
    "print(f\"   Year range: {meta.yearmonth_min} – {meta.yearmonth_max}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 3. Build Antibiotic Flags\n",
    "# \n",
    "# The `DataLoader` generates binary flags for testing (`_T`) and resistance (`_R`) for all antibiotics. This gives us a clean matrix for analysis.\n",
    "\n",
    "# %%\n",
    "all_codes = sorted(loader.code_to_base.keys())\n",
    "print(f\"Total antibiotic codes: {len(all_codes)}\")\n",
    "\n",
    "flags = loader.get_abx_flags(\n",
    "    df,\n",
    "    codes=all_codes,\n",
    "    recode_mode=\"R_vs_nonR\",\n",
    "    drop_I=True,\n",
    ")\n",
    "\n",
    "print(f\"Flags shape: {flags.shape}\")\n",
    "flags.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c1be382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discovery set: 7706 isolates from 38 labs\n",
      "Estimation set: 13756 isolates from 39 labs\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. Split Data into Discovery and Estimation Sets\n",
    "# \n",
    "# We split by laboratory (`Anonymized_Lab`) so that all isolates from a given lab stay together. This prevents information leakage between the two sets.\n",
    "# \n",
    "# **Why split by lab?**  \n",
    "# Testing protocols vary between laboratories. By splitting at the lab level, the routine policy learned on discovery labs is evaluated on entirely different labs, mimicking external validation.\n",
    "# \n",
    "# **What if you use a random split?**  \n",
    "# Then isolates from the same lab could appear in both discovery and estimation, causing the screening to be overly optimistic (winner’s curse) and the policy to be evaluated in‑sample.\n",
    "\n",
    "# %%\n",
    "group_col = \"Anonymized_Lab\"\n",
    "groups = df[group_col].astype(str).fillna(\"NA\").to_numpy()\n",
    "\n",
    "gss = GroupShuffleSplit(n_splits=1, test_size=0.5, random_state=42)\n",
    "discovery_idx, estimation_idx = next(gss.split(df.index, groups=groups))\n",
    "\n",
    "discovery_df = df.iloc[discovery_idx].copy()\n",
    "discovery_flags = flags.iloc[discovery_idx].copy()\n",
    "estimation_df = df.iloc[estimation_idx].copy()\n",
    "estimation_flags = flags.iloc[estimation_idx].copy()\n",
    "\n",
    "print(f\"Discovery set: {len(discovery_df)} isolates from {discovery_df[group_col].nunique()} labs\")\n",
    "print(f\"Estimation set: {len(estimation_df)} isolates from {estimation_df[group_col].nunique()} labs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1918cc4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1 screening retained 100 pairs.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigger</th>\n",
       "      <th>target</th>\n",
       "      <th>or_unadjusted</th>\n",
       "      <th>or_ci_low</th>\n",
       "      <th>or_ci_high</th>\n",
       "      <th>p_value</th>\n",
       "      <th>delta</th>\n",
       "      <th>p_D_tested_given_A_R</th>\n",
       "      <th>p_D_tested_given_A_S</th>\n",
       "      <th>n_trigger_tested</th>\n",
       "      <th>n_A_R</th>\n",
       "      <th>n_A_S</th>\n",
       "      <th>q_value</th>\n",
       "      <th>significant</th>\n",
       "      <th>abs_log_or</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NFT</td>\n",
       "      <td>DOX</td>\n",
       "      <td>452.873255</td>\n",
       "      <td>27.960618</td>\n",
       "      <td>7335.109102</td>\n",
       "      <td>1.303841e-40</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2213</td>\n",
       "      <td>527</td>\n",
       "      <td>1686</td>\n",
       "      <td>8.139690e-39</td>\n",
       "      <td>True</td>\n",
       "      <td>6.115612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRO</td>\n",
       "      <td>COL</td>\n",
       "      <td>352.548217</td>\n",
       "      <td>21.052441</td>\n",
       "      <td>5903.840125</td>\n",
       "      <td>5.380363e-17</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>0.038168</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4697</td>\n",
       "      <td>393</td>\n",
       "      <td>4304</td>\n",
       "      <td>8.708217e-16</td>\n",
       "      <td>True</td>\n",
       "      <td>5.865187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEP</td>\n",
       "      <td>COL</td>\n",
       "      <td>222.510393</td>\n",
       "      <td>13.066392</td>\n",
       "      <td>3789.177354</td>\n",
       "      <td>6.270156e-12</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>0.048458</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2321</td>\n",
       "      <td>227</td>\n",
       "      <td>2094</td>\n",
       "      <td>5.591955e-11</td>\n",
       "      <td>True</td>\n",
       "      <td>5.404974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPO</td>\n",
       "      <td>COL</td>\n",
       "      <td>198.499538</td>\n",
       "      <td>11.735542</td>\n",
       "      <td>3357.498651</td>\n",
       "      <td>4.318855e-12</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.021700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4852</td>\n",
       "      <td>553</td>\n",
       "      <td>4299</td>\n",
       "      <td>3.931957e-11</td>\n",
       "      <td>True</td>\n",
       "      <td>5.290787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TET</td>\n",
       "      <td>CRO</td>\n",
       "      <td>193.617886</td>\n",
       "      <td>11.642249</td>\n",
       "      <td>3219.986645</td>\n",
       "      <td>1.101263e-16</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.306818</td>\n",
       "      <td>0.0</td>\n",
       "      <td>304</td>\n",
       "      <td>88</td>\n",
       "      <td>216</td>\n",
       "      <td>1.718756e-15</td>\n",
       "      <td>True</td>\n",
       "      <td>5.265887</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trigger target  or_unadjusted  or_ci_low   or_ci_high       p_value  \\\n",
       "0     NFT    DOX     452.873255  27.960618  7335.109102  1.303841e-40   \n",
       "1     CRO    COL     352.548217  21.052441  5903.840125  5.380363e-17   \n",
       "2     CEP    COL     222.510393  13.066392  3789.177354  6.270156e-12   \n",
       "3     CPO    COL     198.499538  11.735542  3357.498651  4.318855e-12   \n",
       "4     TET    CRO     193.617886  11.642249  3219.986645  1.101263e-16   \n",
       "\n",
       "      delta  p_D_tested_given_A_R  p_D_tested_given_A_S  n_trigger_tested  \\\n",
       "0  0.117647              0.117647                   0.0              2213   \n",
       "1  0.038168              0.038168                   0.0              4697   \n",
       "2  0.048458              0.048458                   0.0              2321   \n",
       "3  0.021700              0.021700                   0.0              4852   \n",
       "4  0.306818              0.306818                   0.0               304   \n",
       "\n",
       "   n_A_R  n_A_S       q_value  significant  abs_log_or  \n",
       "0    527   1686  8.139690e-39         True    6.115612  \n",
       "1    393   4304  8.708217e-16         True    5.865187  \n",
       "2    227   2094  5.591955e-11         True    5.404974  \n",
       "3    553   4299  3.931957e-11         True    5.290787  \n",
       "4     88    216  1.718756e-15         True    5.265887  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Phase 1 Screening on the Discovery Set\n",
    "# \n",
    "# We use the `Phase1Screener` to identify the top 100 trigger–target pairs (by absolute log‑odds ratio) that pass crude association tests with FDR correction.\n",
    "# \n",
    "# **Why screening?**  \n",
    "# With many antibiotics, the number of possible pairs is huge (~3,600). Estimating all would be computationally prohibitive and would incur severe multiple testing penalties. Screening selects the most promising pairs for the full causal analysis.\n",
    "# \n",
    "# **What if you skip screening and run all pairs?**  \n",
    "# The pipeline would take much longer, and you would face a massive multiple‑comparison problem. Moreover, many pairs would have insufficient sample sizes, leading to many failed estimates.\n",
    "\n",
    "# %%\n",
    "phase1_cfg = Phase1Config(\n",
    "    min_group=50,\n",
    "    min_trigger_tested=100,\n",
    "    crude_screening_threshold=0.05,\n",
    "    fdr_alpha=0.05,\n",
    "    exclude_targets_equal_trigger=True\n",
    ")\n",
    "\n",
    "screener = Phase1Screener(phase1_cfg)\n",
    "\n",
    "phase1_df = screener.run(\n",
    "    df=discovery_df,\n",
    "    flags=discovery_flags,\n",
    "    all_codes=all_codes,\n",
    "    top_n=100\n",
    ")\n",
    "\n",
    "print(f\"Phase 1 screening retained {len(phase1_df)} pairs.\")\n",
    "if not phase1_df.empty:\n",
    "    display(phase1_df.head())\n",
    "else:\n",
    "    print(\"⚠️ No pairs passed screening. Check group sizes and thresholds.\")\n",
    "\n",
    "# Convert to list of tuples for the pipeline\n",
    "pairs = list(zip(phase1_df[\"trigger\"], phase1_df[\"target\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29a7cc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 6. Configure the Causal Pipeline\n",
    "# \n",
    "# All parameters are centralised in a `RunConfig` object. You can adjust these settings as needed. Here we set the configuration for the **standard model** (no joint selection).\n",
    "\n",
    "# %%\n",
    "config = RunConfig(\n",
    "    split=SplitConfig(\n",
    "        test_size=0.3,\n",
    "        split_group_col=\"Anonymized_Lab\",\n",
    "        random_state=42\n",
    "    ),\n",
    "    covariates=CovariateConfig(\n",
    "        covariate_cols=[\"Anonymized_Lab\", \"ARS_WardType\", \"AgeGroup\", \"Year\"],\n",
    "        min_count=200,\n",
    "        max_levels=25,\n",
    "        drop_first=True\n",
    "    ),\n",
    "    policy=PolicyConfig(\n",
    "        context_cols=[\"Anonymized_Lab\", \"PathogengroupL1\", \"Year\"],\n",
    "        method=\"empirical\",\n",
    "        min_context_n=100,\n",
    "        model_type=\"xgb\",\n",
    "        calibrate=True,\n",
    "        calibration_method=\"isotonic\",\n",
    "        calibration_cv=5\n",
    "    ),\n",
    "    nuisance=NuisanceConfig(\n",
    "        testing_model=\"xgb\",\n",
    "        propensity_model=\"xgb\",\n",
    "        outcome_model=\"xgb\",\n",
    "        calibrate_testing=True,\n",
    "        calibrate_propensity=False,\n",
    "        calibrate_outcome=False,\n",
    "        testing_cv_folds=5,\n",
    "        use_joint_selection=False,   # we will create two copies with different values\n",
    "        random_state=42\n",
    "    ),\n",
    "    tmle=TMLEConfig(\n",
    "        n_folds=5,\n",
    "        min_prob=0.01,\n",
    "        weight_cap_percentile=99.0,\n",
    "        min_tested=200,\n",
    "        min_group=50,\n",
    "        stabilize_weights=True,\n",
    "        n_bootstrap=None,\n",
    "        alpha=0.05\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba1f1029",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 20:39:14,225 - src.controllers.escalation_causal.data.validation - WARNING - Trigger AZM: minimum predicted testing probability among tested = 0.0024 < 0.01\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=4)]: Done  10 tasks      | elapsed:    2.0s\n",
      "[Parallel(n_jobs=4)]: Done  17 tasks      | elapsed:    8.6s\n",
      "[Parallel(n_jobs=4)]: Done  24 tasks      | elapsed:   15.8s\n",
      "[Parallel(n_jobs=4)]: Done  33 tasks      | elapsed:   22.9s\n",
      "[Parallel(n_jobs=4)]: Done  42 tasks      | elapsed:   23.5s\n",
      "[Parallel(n_jobs=4)]: Done  53 tasks      | elapsed:   37.4s\n",
      "[Parallel(n_jobs=4)]: Done  64 tasks      | elapsed:   44.8s\n",
      "[Parallel(n_jobs=4)]: Done  77 tasks      | elapsed:   53.9s\n",
      "[Parallel(n_jobs=4)]: Done  90 tasks      | elapsed:  1.3min\n",
      "[Parallel(n_jobs=4)]: Done 100 out of 100 | elapsed:  1.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of successful estimates (no joint): 45\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trigger</th>\n",
       "      <th>target</th>\n",
       "      <th>rd</th>\n",
       "      <th>ci_low</th>\n",
       "      <th>ci_high</th>\n",
       "      <th>p_value</th>\n",
       "      <th>se</th>\n",
       "      <th>ess</th>\n",
       "      <th>n_used</th>\n",
       "      <th>n_trigger_tested</th>\n",
       "      <th>n_A1</th>\n",
       "      <th>n_A0</th>\n",
       "      <th>baseline_mu0</th>\n",
       "      <th>escalation_score_mean</th>\n",
       "      <th>diagnostics</th>\n",
       "      <th>model_spec</th>\n",
       "      <th>status</th>\n",
       "      <th>skip_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NFT</td>\n",
       "      <td>DOX</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>450.228404</td>\n",
       "      <td>718</td>\n",
       "      <td>718</td>\n",
       "      <td>447</td>\n",
       "      <td>271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'testing_model': {'p_test_quantiles': {'q00':...</td>\n",
       "      <td>{'propensity_model': 'xgb', 'outcome_model': '...</td>\n",
       "      <td>ok</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CRO</td>\n",
       "      <td>COL</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>365</td>\n",
       "      <td>39</td>\n",
       "      <td>326</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>failed</td>\n",
       "      <td>Group sizes too small: A=1 39, A=0 326 (min re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CEP</td>\n",
       "      <td>COL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>487.921173</td>\n",
       "      <td>582</td>\n",
       "      <td>582</td>\n",
       "      <td>60</td>\n",
       "      <td>522</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'testing_model': {'p_test_quantiles': {'q00':...</td>\n",
       "      <td>{'propensity_model': 'xgb', 'outcome_model': '...</td>\n",
       "      <td>ok</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CPO</td>\n",
       "      <td>COL</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>984.205472</td>\n",
       "      <td>1031</td>\n",
       "      <td>1031</td>\n",
       "      <td>107</td>\n",
       "      <td>924</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>{'testing_model': {'p_test_quantiles': {'q00':...</td>\n",
       "      <td>{'propensity_model': 'xgb', 'outcome_model': '...</td>\n",
       "      <td>ok</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TET</td>\n",
       "      <td>CRO</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>83</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>{}</td>\n",
       "      <td>{}</td>\n",
       "      <td>failed</td>\n",
       "      <td>Too few tested isolates for TET: n=83 &lt; 200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  trigger target   rd  ci_low  ci_high  p_value   se         ess  n_used  \\\n",
       "0     NFT    DOX  0.0     0.0      0.0      1.0  0.0  450.228404     718   \n",
       "1     CRO    COL  NaN     NaN      NaN      NaN  NaN         NaN       0   \n",
       "2     CEP    COL  0.0     0.0      0.0      1.0  0.0  487.921173     582   \n",
       "3     CPO    COL  0.0     0.0      0.0      1.0  0.0  984.205472    1031   \n",
       "4     TET    CRO  NaN     NaN      NaN      NaN  NaN         NaN       0   \n",
       "\n",
       "   n_trigger_tested  n_A1  n_A0  baseline_mu0  escalation_score_mean  \\\n",
       "0               718   447   271           0.0                    0.0   \n",
       "1               365    39   326           NaN                    NaN   \n",
       "2               582    60   522           0.0                    0.0   \n",
       "3              1031   107   924           0.0                    0.0   \n",
       "4                83     0     0           NaN                    NaN   \n",
       "\n",
       "                                         diagnostics  \\\n",
       "0  {'testing_model': {'p_test_quantiles': {'q00':...   \n",
       "1                                                 {}   \n",
       "2  {'testing_model': {'p_test_quantiles': {'q00':...   \n",
       "3  {'testing_model': {'p_test_quantiles': {'q00':...   \n",
       "4                                                 {}   \n",
       "\n",
       "                                          model_spec  status  \\\n",
       "0  {'propensity_model': 'xgb', 'outcome_model': '...      ok   \n",
       "1                                                 {}  failed   \n",
       "2  {'propensity_model': 'xgb', 'outcome_model': '...      ok   \n",
       "3  {'propensity_model': 'xgb', 'outcome_model': '...      ok   \n",
       "4                                                 {}  failed   \n",
       "\n",
       "                                         skip_reason  \n",
       "0                                                     \n",
       "1  Group sizes too small: A=1 39, A=0 326 (min re...  \n",
       "2                                                     \n",
       "3                                                     \n",
       "4        Too few tested isolates for TET: n=83 < 200  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 7. Run Pipeline Without Joint Selection (Standard Model)\n",
    "# \n",
    "# This is our baseline – it uses standard inverse probability weighting for testing selection.\n",
    "\n",
    "# %%\n",
    "config_no_joint = config.model_copy(deep=True)\n",
    "config_no_joint.nuisance.use_joint_selection = False\n",
    "\n",
    "pipeline_no_joint = CausalPipeline(config_no_joint, n_jobs=4)   # use 4 cores\n",
    "results_no_joint = pipeline_no_joint.run(\n",
    "    df=estimation_df,\n",
    "    flags=estimation_flags,\n",
    "    all_codes=all_codes,\n",
    "    pairs=pairs\n",
    ")\n",
    "\n",
    "print(f\"Number of successful estimates (no joint): {len(results_no_joint[results_no_joint['status']=='ok'])}\")\n",
    "results_no_joint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dc41c441",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-17 20:41:27,353 - src.controllers.escalation_causal.data.validation - WARNING - Trigger AZM: minimum predicted testing probability among tested = 0.0024 < 0.01\n",
      "[Parallel(n_jobs=4)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "[Parallel(n_jobs=4)]: Batch computation too fast (0.09719705581665039s.) Setting batch_size=2.\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Done   5 tasks      | elapsed:    0.3s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Done  13 tasks      | elapsed:    2.3s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Batch computation too slow (2.3003696411285404s.) Setting batch_size=1.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Done  26 tasks      | elapsed:    6.1s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Done  38 tasks      | elapsed:    7.2s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "[Parallel(n_jobs=4)]: Done  49 tasks      | elapsed:   12.7s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "[Parallel(n_jobs=4)]: Done  58 tasks      | elapsed:   15.9s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "[Parallel(n_jobs=4)]: Done  69 tasks      | elapsed:   20.3s\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n",
      "Joint model fitting failed: Singular matrix. Falling back to separate probits.\n",
      "/Users/awotoroebenezer/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/statsmodels/base/model.py:607: ConvergenceWarning: Maximum Likelihood optimization failed to converge. Check mle_retvals\n",
      "  warnings.warn(\"Maximum Likelihood optimization failed to \"\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      8\u001b[39m config_joint.nuisance.use_joint_selection = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m     10\u001b[39m pipeline_joint = CausalPipeline(config_joint, n_jobs=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m results_joint = \u001b[43mpipeline_joint\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdf\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimation_df\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimation_flags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_codes\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_codes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpairs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpairs\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNumber of successful estimates (joint): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(results_joint[results_joint[\u001b[33m'\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m'\u001b[39m]==\u001b[33m'\u001b[39m\u001b[33mok\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     19\u001b[39m results_joint.head()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cascading_testing_protocol/src/controllers/escalation_causal/pipeline.py:143\u001b[39m, in \u001b[36mCausalPipeline.run\u001b[39m\u001b[34m(self, df, flags, all_codes, pairs)\u001b[39m\n\u001b[32m    140\u001b[39m         esc_scores[code] = scores\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# 5. Process each pair in parallel\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m143\u001b[39m results = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    144\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_estimate_pair_wrapper\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mesc_scores\u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrigger\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpairs\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    150\u001b[39m df_out = pd.DataFrame([\u001b[38;5;28mvars\u001b[39m(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m results])\n\u001b[32m    151\u001b[39m \u001b[38;5;28mself\u001b[39m._results = df_out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/joblib/parallel.py:2072\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2066\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2067\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2068\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2069\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2070\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2072\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/joblib/parallel.py:1682\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1679\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1681\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1682\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1684\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1685\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1686\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1687\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1688\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/cascading_testing_protocol/.venv/lib/python3.12/site-packages/joblib/parallel.py:1800\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1789\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_ordered:\n\u001b[32m   1790\u001b[39m     \u001b[38;5;66;03m# Case ordered: wait for completion (or error) of the next job\u001b[39;00m\n\u001b[32m   1791\u001b[39m     \u001b[38;5;66;03m# that have been dispatched and not retrieved yet. If no job\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1795\u001b[39m     \u001b[38;5;66;03m# control only have to be done on the amount of time the next\u001b[39;00m\n\u001b[32m   1796\u001b[39m     \u001b[38;5;66;03m# dispatched job is pending.\u001b[39;00m\n\u001b[32m   1797\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (nb_jobs == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   1798\u001b[39m         \u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING\n\u001b[32m   1799\u001b[39m     ):\n\u001b[32m-> \u001b[39m\u001b[32m1800\u001b[39m         \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1801\u001b[39m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1803\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m nb_jobs == \u001b[32m0\u001b[39m:\n\u001b[32m   1804\u001b[39m     \u001b[38;5;66;03m# Case unordered: jobs are added to the list of jobs to\u001b[39;00m\n\u001b[32m   1805\u001b[39m     \u001b[38;5;66;03m# retrieve `self._jobs` only once completed or in error, which\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1811\u001b[39m     \u001b[38;5;66;03m# timeouts before any other dispatched job has completed and\u001b[39;00m\n\u001b[32m   1812\u001b[39m     \u001b[38;5;66;03m# been added to `self._jobs` to be retrieved.\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 8. Run Pipeline With Joint Selection\n",
    "# \n",
    "# Now we enable the bivariate probit joint selection model to account for unmeasured common causes of testing and resistance.\n",
    "\n",
    "# %%\n",
    "config_joint = config.model_copy(deep=True)\n",
    "config_joint.nuisance.use_joint_selection = True\n",
    "\n",
    "pipeline_joint = CausalPipeline(config_joint, n_jobs=4)\n",
    "results_joint = pipeline_joint.run(\n",
    "    df=estimation_df,\n",
    "    flags=estimation_flags,\n",
    "    all_codes=all_codes,\n",
    "    pairs=pairs\n",
    ")\n",
    "\n",
    "print(f\"Number of successful estimates (joint): {len(results_joint[results_joint['status']=='ok'])}\")\n",
    "results_joint.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43586a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 9. Compare the Two Sets of Results\n",
    "# \n",
    "# We merge the two result DataFrames on trigger–target pairs and compute the difference.\n",
    "\n",
    "# %%\n",
    "def compare_results(res_no_joint, res_joint):\n",
    "    # Keep only successful estimates\n",
    "    noj_ok = res_no_joint[res_no_joint[\"status\"] == \"ok\"][[\"trigger\", \"target\", \"rd\", \"ci_low\", \"ci_high\"]].copy()\n",
    "    joint_ok = res_joint[res_joint[\"status\"] == \"ok\"][[\"trigger\", \"target\", \"rd\", \"ci_low\", \"ci_high\"]].copy()\n",
    "    \n",
    "    noj_ok.rename(columns={\"rd\": \"rd_noj\", \"ci_low\": \"ci_low_noj\", \"ci_high\": \"ci_high_noj\"}, inplace=True)\n",
    "    joint_ok.rename(columns={\"rd\": \"rd_joint\", \"ci_low\": \"ci_low_joint\", \"ci_high\": \"ci_high_joint\"}, inplace=True)\n",
    "    \n",
    "    merged = pd.merge(noj_ok, joint_ok, on=[\"trigger\", \"target\"], how=\"inner\")\n",
    "    merged[\"diff\"] = merged[\"rd_joint\"] - merged[\"rd_noj\"]\n",
    "    merged[\"pair\"] = merged[\"trigger\"] + \" → \" + merged[\"target\"]\n",
    "    return merged\n",
    "\n",
    "comparison_df = compare_results(results_no_joint, results_joint)\n",
    "print(f\"Merged {len(comparison_df)} pairs for comparison.\")\n",
    "comparison_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771ce620",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. Visualise the Comparison\n",
    "# \n",
    "# We create a two‑panel plot: point estimates with confidence intervals for both methods (top), and a bar chart of the differences (bottom).\n",
    "\n",
    "# %%\n",
    "def plot_comparison(merged):\n",
    "    merged_sorted = merged.sort_values(\"diff\", ascending=False).reset_index(drop=True)\n",
    "    y_pos = list(range(len(merged_sorted)))\n",
    "    \n",
    "    fig = make_subplots(\n",
    "        rows=2, cols=1,\n",
    "        subplot_titles=(\"Risk Difference Estimates\", \"Difference (Joint – Standard)\"),\n",
    "        shared_xaxes=True,\n",
    "        vertical_spacing=0.15\n",
    "    )\n",
    "    \n",
    "    # Standard estimates\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=merged_sorted[\"rd_noj\"],\n",
    "        y=y_pos,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"blue\", size=8),\n",
    "        name=\"Standard\",\n",
    "        error_x=dict(\n",
    "            type=\"data\",\n",
    "            symmetric=False,\n",
    "            array=merged_sorted[\"rd_noj\"] - merged_sorted[\"ci_low_noj\"],\n",
    "            arrayminus=merged_sorted[\"ci_high_noj\"] - merged_sorted[\"rd_noj\"],\n",
    "            color=\"blue\",\n",
    "            thickness=1\n",
    "        ),\n",
    "        showlegend=True\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Joint estimates\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=merged_sorted[\"rd_joint\"],\n",
    "        y=y_pos,\n",
    "        mode=\"markers\",\n",
    "        marker=dict(color=\"red\", size=8),\n",
    "        name=\"Joint\",\n",
    "        error_x=dict(\n",
    "            type=\"data\",\n",
    "            symmetric=False,\n",
    "            array=merged_sorted[\"rd_joint\"] - merged_sorted[\"ci_low_joint\"],\n",
    "            arrayminus=merged_sorted[\"ci_high_joint\"] - merged_sorted[\"rd_joint\"],\n",
    "            color=\"red\",\n",
    "            thickness=1\n",
    "        ),\n",
    "        showlegend=True\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Difference bar chart\n",
    "    colors = [\"red\" if d > 0 else \"blue\" for d in merged_sorted[\"diff\"]]\n",
    "    fig.add_trace(go.Bar(\n",
    "        x=merged_sorted[\"diff\"],\n",
    "        y=y_pos,\n",
    "        orientation=\"h\",\n",
    "        marker_color=colors,\n",
    "        name=\"Difference\",\n",
    "        showlegend=False\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    # Vertical lines at zero\n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", row=1, col=1)\n",
    "    fig.add_vline(x=0, line_dash=\"dash\", line_color=\"gray\", row=2, col=1)\n",
    "    \n",
    "    fig.update_yaxes(tickvals=y_pos, ticktext=merged_sorted[\"pair\"], row=1, col=1)\n",
    "    fig.update_yaxes(tickvals=y_pos, ticktext=merged_sorted[\"pair\"], row=2, col=1)\n",
    "    \n",
    "    fig.update_layout(\n",
    "        height=800,\n",
    "        width=1000,\n",
    "        title_text=\"Comparison: Standard vs. Joint Selection Model\",\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "plot_comparison(comparison_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e81bf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 11. Save Intermediate Results (Optional)\n",
    "# \n",
    "# Save the comparison data and the individual results for later use.\n",
    "\n",
    "# %%\n",
    "output_dir = Path(\"./comparison_output\")\n",
    "output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "comparison_df.to_csv(output_dir / \"comparison_results.csv\", index=False)\n",
    "results_no_joint.to_csv(output_dir / \"results_no_joint.csv\", index=False)\n",
    "results_joint.to_csv(output_dir / \"results_joint.csv\", index=False)\n",
    "\n",
    "print(f\"Results saved to {output_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4817a243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 12. Bayesian Shrinkage for Multiple Comparisons\n",
    "# \n",
    "# We apply a Bayesian hierarchical model to shrink the estimates toward a common mean, borrowing strength across pairs. This helps guard against false positives due to multiple testing.\n",
    "# \n",
    "# **Important**: We must exclude pairs with zero standard error (where the target was never tested) because they cause degeneracy in the likelihood.\n",
    "\n",
    "# %%\n",
    "# Load the standard model results (we can also use joint, but let's use standard for consistency)\n",
    "results_std = pd.read_csv(output_dir / \"results_no_joint.csv\")\n",
    "\n",
    "# Keep only successful estimates with positive standard errors\n",
    "ok = results_std[(results_std['status'] == 'ok') & (results_std['se'] > 0)].copy()\n",
    "ok['pair'] = ok['trigger'] + ' → ' + ok['target']\n",
    "ok = ok.dropna(subset=['rd', 'se'])\n",
    "\n",
    "if len(ok) > 1:\n",
    "    # Use higher target_accept to reduce divergences; may need to tune further\n",
    "    bs = BayesianShrinkage(random_seed=42, draws=2000, tune=1000, target_accept=0.99)\n",
    "    bs.fit(ok, estimate_col='rd', se_col='se')\n",
    "    shrunk = bs.summary()\n",
    "    shrunk.to_csv(output_dir / \"bayesian_shrinkage_summary.csv\", index=False)\n",
    "    print(f\"Shrinkage complete for {len(ok)} pairs.\")\n",
    "    display(shrunk.head())\n",
    "else:\n",
    "    print(\"Not enough valid pairs for Bayesian shrinkage.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daa38ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# **Interpretation**: \n",
    "# - `theta_shrunken` is the posterior mean after shrinkage.\n",
    "# - `prob_positive` is the posterior probability that the true effect is positive.\n",
    "# - If `prob_positive` > 0.95, we have strong evidence of a positive effect.\n",
    "# \n",
    "# *Note*: MCMC warnings (divergences, rhat > 1.01) may appear; they indicate that the posterior may not be fully reliable. You can try increasing `target_accept` further or using a more informative prior for `tau` (modify the `BayesianShrinkage` class).\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 13. Heterogeneity Analysis with Causal Forest (for a Pair of Interest)\n",
    "# \n",
    "# Suppose we want to explore whether the effect varies by clinical context for a specific pair – e.g., the one with the strongest signal. From our results, we might choose **NFT → ERT** (p ≈ 3×10⁻⁵) or **CPO → CIP** (high posterior probability, though tiny effect). We'll use **NFT → ERT** for illustration.\n",
    "# \n",
    "# **Challenge**: The pipeline does not automatically store per‑observation data. We need to reconstruct them for the pair of interest. The following steps do this by:\n",
    "# - Extracting the test set from the pipeline (or re‑running the pipeline for a single pair with a modified version that returns the data).\n",
    "# - Re‑fitting the testing model to obtain weights.\n",
    "# - Fitting a causal forest.\n",
    "\n",
    "# %%\n",
    "# Choose a pair\n",
    "trigger = 'NFT'\n",
    "target = 'ERT'\n",
    "\n",
    "# We need the test set from the pipeline run. If you saved it, load it; otherwise, we can re‑extract from estimation set.\n",
    "# For simplicity, we assume the pipeline object `pipeline_no_joint` still exists and has attributes `df_test`, `flags_test`, `esc_scores`.\n",
    "# If not, you may need to re‑run the pipeline for this single pair with a modified version that stores these.\n",
    "\n",
    "# Check if the pipeline has the necessary attributes (you may need to modify pipeline.py to store them)\n",
    "if hasattr(pipeline_no_joint, 'df_test') and hasattr(pipeline_no_joint, 'flags_test') and hasattr(pipeline_no_joint, 'esc_scores'):\n",
    "    df_test = pipeline_no_joint.df_test\n",
    "    flags_test = pipeline_no_joint.flags_test\n",
    "    esc_scores = pipeline_no_joint.esc_scores\n",
    "else:\n",
    "    # Fallback: re‑run the pipeline for just this pair with a modified config that ensures we get the test set\n",
    "    print(\"Pipeline does not store test data; re‑running for the single pair with a custom function.\")\n",
    "    # (This would require a custom function; for brevity, we assume the data are available.)\n",
    "\n",
    "# Get tested mask for trigger\n",
    "T_col = f\"{trigger}_T\"\n",
    "tested_mask = flags_test[T_col].astype(int).to_numpy() == 1\n",
    "\n",
    "# Build covariates (using same encoding as pipeline)\n",
    "X = pipeline_no_joint._encode_covariates(df_test, config.covariates.covariate_cols)\n",
    "X_tested = X[tested_mask]\n",
    "\n",
    "# Treatment and outcome\n",
    "A = flags_test[f\"{trigger}_R\"].astype(int).to_numpy()[tested_mask]\n",
    "Y = esc_scores[target][tested_mask]\n",
    "\n",
    "# Re‑compute testing model weights for this pair (using standard model, since joint may have issues)\n",
    "from src.controllers.escalation_causal.nuisance.testing_model import TestingModel\n",
    "test_model = TestingModel(\n",
    "    model_type=config.nuisance.testing_model,\n",
    "    calibrate=config.nuisance.calibrate_testing,\n",
    "    n_folds_cv=config.nuisance.testing_cv_folds,\n",
    "    min_prob=config.tmle.min_prob,\n",
    "    weight_cap_percentile=config.tmle.weight_cap_percentile,\n",
    ")\n",
    "T_full = flags_test[T_col].astype(int).to_numpy()\n",
    "test_model.fit(X, T_full)\n",
    "p_test = test_model.get_oof_predictions() if test_model._is_cross_fitted else test_model.predict_proba(X)\n",
    "w_full, _ = test_model.compute_weights(p_test, tested_mask)\n",
    "w = w_full[tested_mask]\n",
    "\n",
    "# Fit causal forest\n",
    "# We need feature names (you can extract them from the encoder; here we use generic names)\n",
    "feature_names = [f\"cov_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "cf = CausalForestWrapper(\n",
    "    n_estimators=400,\n",
    "    max_depth=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "cf.fit(X_tested, A, Y, sample_weight=w, feature_names=feature_names)\n",
    "\n",
    "# Variable importance\n",
    "imp = cf.feature_importances()\n",
    "imp_df = pd.DataFrame(list(imp.items()), columns=['feature', 'importance'])\n",
    "imp_df.sort_values('importance', ascending=False).to_csv(output_dir / f\"cf_importance_{trigger}_{target}.csv\", index=False)\n",
    "print(\"Variable importance saved.\")\n",
    "\n",
    "# Subgroup summaries\n",
    "# By ward type\n",
    "ward_labels = df_test.loc[tested_mask, \"ARS_WardType\"].values\n",
    "ward_summary = cf.get_cate_summary(X_tested, group_labels=ward_labels)\n",
    "ward_summary.to_csv(output_dir / f\"cate_by_ward_{trigger}_{target}.csv\", index=False)\n",
    "print(\"CATE by ward type:\")\n",
    "print(ward_summary)\n",
    "\n",
    "# By age group\n",
    "age_labels = df_test.loc[tested_mask, \"AgeGroup\"].values\n",
    "age_summary = cf.get_cate_summary(X_tested, group_labels=age_labels)\n",
    "age_summary.to_csv(output_dir / f\"cate_by_age_{trigger}_{target}.csv\", index=False)\n",
    "print(\"CATE by age group:\")\n",
    "print(age_summary)\n",
    "\n",
    "# Plot CATE distribution\n",
    "plt.figure()\n",
    "cf.plot_cate_distribution(X_tested)\n",
    "plt.savefig(output_dir / f\"cate_distribution_{trigger}_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Plot variable importance (top 15)\n",
    "cf.plot_variable_importance(top_k=15)\n",
    "plt.savefig(output_dir / f\"cf_importance_plot_{trigger}_{target}.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4dcfcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# **Interpretation**:\n",
    "# - Variable importance shows which covariates (ward type, age, year) most influence the treatment effect.\n",
    "# - Subgroup summaries give average CATE within each category. If the confidence intervals are wide, the heterogeneity may not be statistically significant.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 14. Summary and Next Steps\n",
    "# \n",
    "# You have now:\n",
    "# 1. Loaded and filtered the ARS data.\n",
    "# 2. Split into discovery and estimation sets.\n",
    "# 3. Performed Phase 1 screening to select 100 trigger–target pairs.\n",
    "# 4. Run the causal pipeline with and without the joint selection model.\n",
    "# 5. Compared the estimates and generated a comparison plot.\n",
    "# 6. Applied Bayesian shrinkage to handle multiple comparisons.\n",
    "# 7. Explored heterogeneity with a causal forest for a specific pair.\n",
    "# \n",
    "# **Possible next steps**:\n",
    "# - Re‑run with a lower `min_group` threshold to include more pairs.\n",
    "# - Improve the joint model convergence by tweaking its parameters.\n",
    "# - Generate publication‑ready figures using the `visualization` module (forest plots, network graphs, etc.).\n",
    "# - Write up your findings in a paper.\n",
    "\n",
    "# %% [markdown]\n",
    "# ## 15. Troubleshooting\n",
    "# \n",
    "# | Problem | Likely cause | Solution |\n",
    "# |---------|--------------|----------|\n",
    "# | Phase 1 screening empty | `min_group` or `min_trigger_tested` too high | Lower thresholds, or check data. |\n",
    "# | Pipeline fails for many pairs | `min_tested` or `min_group` too high; some triggers rarely tested | Reduce thresholds, or accept that some pairs cannot be estimated. |\n",
    "# | Joint model fails to converge | Sample size too small; perfect prediction | Fallback to separate models automatically; consider using a two‑stage Heckman instead. |\n",
    "# | Bayesian shrinkage gives warnings (divergences, rhat) | Weak prior, small number of pairs | Increase `target_accept`, use more informative prior for tau, or increase draws. |\n",
    "# | Causal forest requires test set data | Pipeline not configured to store it | Modify `pipeline.py` to save `df_test`, `flags_test`, and `esc_scores` after run. |\n",
    "\n",
    "# %% [markdown]\n",
    "# ---\n",
    "# **End of notebook**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
